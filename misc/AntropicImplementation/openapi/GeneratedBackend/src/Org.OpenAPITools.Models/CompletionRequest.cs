/*
 * Anthropic API
 *
 * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)
 *
 * The version of the OpenAPI document: 0.0.0
 * 
 * Generated by: https://openapi-generator.tech
 */

using System;
using System.Linq;
using System.Text;
using System.Collections.Generic;
using System.ComponentModel;
using System.ComponentModel.DataAnnotations;
using System.Runtime.Serialization;
using System.Text.Json;
using Org.OpenAPITools.Converters;

namespace Org.OpenAPITools.Models
{ 
    /// <summary>
    /// 
    /// </summary>
    [DataContract]
    public partial class CompletionRequest 
    {
        /// <summary>
        /// Gets or Sets Model
        /// </summary>
        [Required]
        [DataMember(Name="model", EmitDefaultValue=false)]
        public Model Model { get; set; }

        /// <summary>
        /// The prompt that you want Claude to complete.  For proper response generation you will need to format your prompt using alternating &#x60;\\n\\nHuman:&#x60; and &#x60;\\n\\nAssistant:&#x60; conversational turns. For example:  &#x60;&#x60;&#x60; \&quot;\\n\\nHuman: {userQuestion}\\n\\nAssistant:\&quot; &#x60;&#x60;&#x60;  See [prompt validation](https://docs.anthropic.com/en/api/prompt-validation) and our guide to [prompt design](https://docs.anthropic.com/en/docs/intro-to-prompting) for more details.
        /// </summary>
        /// <value>The prompt that you want Claude to complete.  For proper response generation you will need to format your prompt using alternating &#x60;\\n\\nHuman:&#x60; and &#x60;\\n\\nAssistant:&#x60; conversational turns. For example:  &#x60;&#x60;&#x60; \&quot;\\n\\nHuman: {userQuestion}\\n\\nAssistant:\&quot; &#x60;&#x60;&#x60;  See [prompt validation](https://docs.anthropic.com/en/api/prompt-validation) and our guide to [prompt design](https://docs.anthropic.com/en/docs/intro-to-prompting) for more details.</value>
        [Required]
        [MinLength(1)]
        [DataMember(Name="prompt", EmitDefaultValue=false)]
        public string Prompt { get; set; }

        /// <summary>
        /// The maximum number of tokens to generate before stopping.  Note that our models may stop _before_ reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.
        /// </summary>
        /// <value>The maximum number of tokens to generate before stopping.  Note that our models may stop _before_ reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.</value>
        [Required]
        [DataMember(Name="max_tokens_to_sample", EmitDefaultValue=true)]
        public int MaxTokensToSample { get; set; }

        /// <summary>
        /// Sequences that will cause the model to stop generating.  Our models stop on &#x60;\&quot;\\n\\nHuman:\&quot;&#x60;, and may include additional built-in stop sequences in the future. By providing the stop_sequences parameter, you may include additional strings that will cause the model to stop generating.
        /// </summary>
        /// <value>Sequences that will cause the model to stop generating.  Our models stop on &#x60;\&quot;\\n\\nHuman:\&quot;&#x60;, and may include additional built-in stop sequences in the future. By providing the stop_sequences parameter, you may include additional strings that will cause the model to stop generating.</value>
        [DataMember(Name="stop_sequences", EmitDefaultValue=false)]
        public List<string> StopSequences { get; set; }

        /// <summary>
        /// Amount of randomness injected into the response.  Defaults to &#x60;1.0&#x60;. Ranges from &#x60;0.0&#x60; to &#x60;1.0&#x60;. Use &#x60;temperature&#x60; closer to &#x60;0.0&#x60; for analytical / multiple choice, and closer to &#x60;1.0&#x60; for creative and generative tasks.  Note that even with &#x60;temperature&#x60; of &#x60;0.0&#x60;, the results will not be fully deterministic.
        /// </summary>
        /// <value>Amount of randomness injected into the response.  Defaults to &#x60;1.0&#x60;. Ranges from &#x60;0.0&#x60; to &#x60;1.0&#x60;. Use &#x60;temperature&#x60; closer to &#x60;0.0&#x60; for analytical / multiple choice, and closer to &#x60;1.0&#x60; for creative and generative tasks.  Note that even with &#x60;temperature&#x60; of &#x60;0.0&#x60;, the results will not be fully deterministic.</value>
        [Range(0, 1)]
        [DataMember(Name="temperature", EmitDefaultValue=true)]
        public decimal? Temperature { get; set; }

        /// <summary>
        /// Use nucleus sampling.  In nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by &#x60;top_p&#x60;. You should either alter &#x60;temperature&#x60; or &#x60;top_p&#x60;, but not both.  Recommended for advanced use cases only. You usually only need to use &#x60;temperature&#x60;.
        /// </summary>
        /// <value>Use nucleus sampling.  In nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by &#x60;top_p&#x60;. You should either alter &#x60;temperature&#x60; or &#x60;top_p&#x60;, but not both.  Recommended for advanced use cases only. You usually only need to use &#x60;temperature&#x60;.</value>
        [Range(0, 1)]
        [DataMember(Name="top_p", EmitDefaultValue=true)]
        public decimal? TopP { get; set; }

        /// <summary>
        /// Only sample from the top K options for each subsequent token.  Used to remove \&quot;long tail\&quot; low probability responses. [Learn more technical details here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).  Recommended for advanced use cases only. You usually only need to use &#x60;temperature&#x60;.
        /// </summary>
        /// <value>Only sample from the top K options for each subsequent token.  Used to remove \&quot;long tail\&quot; low probability responses. [Learn more technical details here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).  Recommended for advanced use cases only. You usually only need to use &#x60;temperature&#x60;.</value>
        [DataMember(Name="top_k", EmitDefaultValue=true)]
        public int? TopK { get; set; }

        /// <summary>
        /// An object describing metadata about the request.
        /// </summary>
        /// <value>An object describing metadata about the request.</value>
        [DataMember(Name="metadata", EmitDefaultValue=false)]
        public Metadata? Metadata { get; set; }

        /// <summary>
        /// Whether to incrementally stream the response using server-sent events.  See [streaming](https://docs.anthropic.com/en/api/streaming) for details.
        /// </summary>
        /// <value>Whether to incrementally stream the response using server-sent events.  See [streaming](https://docs.anthropic.com/en/api/streaming) for details.</value>
        [DataMember(Name="stream", EmitDefaultValue=true)]
        public bool? Stream { get; set; }

    }
}
