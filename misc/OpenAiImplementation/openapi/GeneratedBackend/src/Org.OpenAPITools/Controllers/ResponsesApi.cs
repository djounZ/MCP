/*
 * OpenAI API
 *
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * The version of the OpenAPI document: 2.3.0
 * 
 * Generated by: https://openapi-generator.tech
 */

using System;
using System.Collections.Generic;
using System.ComponentModel.DataAnnotations;
using Microsoft.AspNetCore.Authorization;
using Microsoft.AspNetCore.Mvc;
using Microsoft.AspNetCore.Http;
using System.Text.Json;
using Org.OpenAPITools.Attributes;
using Org.OpenAPITools.Models;

namespace Org.OpenAPITools.Controllers
{ 
    /// <summary>
    /// 
    /// </summary>
    [ApiController]
    public class ResponsesApiController : ControllerBase
    { 
        /// <summary>
        /// Cancel a response
        /// </summary>
        /// <remarks>Cancels a model response with the given ID. Only responses created with the &#x60;background&#x60; parameter set to &#x60;true&#x60; can be cancelled.  [Learn more](https://platform.openai.com/docs/guides/background). </remarks>
        /// <param name="responseId">The ID of the response to cancel.</param>
        /// <response code="200">OK</response>
        /// <response code="404">Not Found</response>
        [HttpPost]
        [Route("/v1/responses/{response_id}/cancel")]
        [Authorize]
        [ValidateModelState]
        [ProducesResponseType(statusCode: 200, type: typeof(Response))]
        [ProducesResponseType(statusCode: 404, type: typeof(Error))]
        public virtual IActionResult CancelResponse([FromRoute (Name = "response_id")][Required]string responseId)
        {

            //TODO: Uncomment the next line to return response 200 or use other options such as return this.NotFound(), return this.BadRequest(..), ...
            // return StatusCode(200, default);
            //TODO: Uncomment the next line to return response 404 or use other options such as return this.NotFound(), return this.BadRequest(..), ...
            // return StatusCode(404, default);
            string exampleJson = null;
            exampleJson = "{\r\n  \"top_logprobs\" : 1,\r\n  \"instructions\" : \"Response_allOf_instructions\",\r\n  \"metadata\" : {\r\n    \"key\" : \"metadata\"\r\n  },\r\n  \"max_tool_calls\" : 1,\r\n  \"reasoning\" : {\r\n    \"summary\" : \"auto\",\r\n    \"effort\" : \"medium\",\r\n    \"generate_summary\" : \"auto\"\r\n  },\r\n  \"usage\" : {\r\n    \"input_tokens_details\" : {\r\n      \"cached_tokens\" : 4\r\n    },\r\n    \"total_tokens\" : 1,\r\n    \"output_tokens\" : 7,\r\n    \"input_tokens\" : 2,\r\n    \"output_tokens_details\" : {\r\n      \"reasoning_tokens\" : 1\r\n    }\r\n  },\r\n  \"created_at\" : 5.962133916683182,\r\n  \"safety_identifier\" : \"safety-identifier-1234\",\r\n  \"error\" : {\r\n    \"code\" : \"server_error\",\r\n    \"message\" : \"message\"\r\n  },\r\n  \"tools\" : [ {\r\n    \"name\" : \"name\",\r\n    \"description\" : \"description\",\r\n    \"type\" : \"function\",\r\n    \"strict\" : true,\r\n    \"parameters\" : {\r\n      \"key\" : \"\"\r\n    }\r\n  }, {\r\n    \"name\" : \"name\",\r\n    \"description\" : \"description\",\r\n    \"type\" : \"function\",\r\n    \"strict\" : true,\r\n    \"parameters\" : {\r\n      \"key\" : \"\"\r\n    }\r\n  } ],\r\n  \"top_p\" : 1,\r\n  \"output\" : [ {\r\n    \"role\" : \"assistant\",\r\n    \"id\" : \"id\",\r\n    \"type\" : \"message\",\r\n    \"content\" : [ {\r\n      \"annotations\" : [ {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      }, {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      } ],\r\n      \"text\" : \"text\",\r\n      \"type\" : \"output_text\",\r\n      \"logprobs\" : [ {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      }, {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      } ]\r\n    }, {\r\n      \"annotations\" : [ {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      }, {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      } ],\r\n      \"text\" : \"text\",\r\n      \"type\" : \"output_text\",\r\n      \"logprobs\" : [ {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      }, {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      } ]\r\n    } ],\r\n    \"status\" : \"in_progress\"\r\n  }, {\r\n    \"role\" : \"assistant\",\r\n    \"id\" : \"id\",\r\n    \"type\" : \"message\",\r\n    \"content\" : [ {\r\n      \"annotations\" : [ {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      }, {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      } ],\r\n      \"text\" : \"text\",\r\n      \"type\" : \"output_text\",\r\n      \"logprobs\" : [ {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      }, {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      } ]\r\n    }, {\r\n      \"annotations\" : [ {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      }, {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      } ],\r\n      \"text\" : \"text\",\r\n      \"type\" : \"output_text\",\r\n      \"logprobs\" : [ {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      }, {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      } ]\r\n    } ],\r\n    \"status\" : \"in_progress\"\r\n  } ],\r\n  \"previous_response_id\" : \"previous_response_id\",\r\n  \"temperature\" : 1,\r\n  \"tool_choice\" : \"none\",\r\n  \"service_tier\" : \"auto\",\r\n  \"model\" : \"gpt-4o\",\r\n  \"text\" : {\r\n    \"format\" : {\r\n      \"type\" : \"text\"\r\n    },\r\n    \"verbosity\" : \"medium\"\r\n  },\r\n  \"id\" : \"id\",\r\n  \"prompt_cache_key\" : \"prompt-cache-key-1234\",\r\n  \"truncation\" : \"disabled\",\r\n  \"incomplete_details\" : {\r\n    \"reason\" : \"max_output_tokens\"\r\n  },\r\n  \"parallel_tool_calls\" : true,\r\n  \"background\" : false,\r\n  \"output_text\" : \"output_text\",\r\n  \"user\" : \"user-1234\",\r\n  \"prompt\" : {\r\n    \"variables\" : {\r\n      \"key\" : \"ResponsePromptVariables_value\"\r\n    },\r\n    \"id\" : \"id\",\r\n    \"version\" : \"version\"\r\n  },\r\n  \"max_output_tokens\" : 6,\r\n  \"object\" : \"response\",\r\n  \"status\" : \"completed\"\r\n}";
            exampleJson = "{\r\n  \"code\" : \"code\",\r\n  \"param\" : \"param\",\r\n  \"message\" : \"message\",\r\n  \"type\" : \"type\"\r\n}";
            
            var example = exampleJson != null
            ? JsonSerializer.Deserialize<Response>(exampleJson)
            : default;
            //TODO: Change the data returned
            return new ObjectResult(example);
        }

        /// <summary>
        /// Create a model response
        /// </summary>
        /// <remarks>Creates a model response. Provide [text](https://platform.openai.com/docs/guides/text) or [image](https://platform.openai.com/docs/guides/images) inputs to generate [text](https://platform.openai.com/docs/guides/text) or [JSON](https://platform.openai.com/docs/guides/structured-outputs) outputs. Have the model call your own [custom code](https://platform.openai.com/docs/guides/function-calling) or use built-in [tools](https://platform.openai.com/docs/guides/tools) like [web search](https://platform.openai.com/docs/guides/tools-web-search) or [file search](https://platform.openai.com/docs/guides/tools-file-search) to use your own data as input for the model&#39;s response. </remarks>
        /// <param name="createResponse"></param>
        /// <response code="200">OK</response>
        [HttpPost]
        [Route("/v1/responses")]
        [Authorize]
        [Consumes("application/json")]
        [ValidateModelState]
        [ProducesResponseType(statusCode: 200, type: typeof(Response))]
        public virtual IActionResult CreateResponse([FromBody]CreateResponse createResponse)
        {

            //TODO: Uncomment the next line to return response 200 or use other options such as return this.NotFound(), return this.BadRequest(..), ...
            // return StatusCode(200, default);
            string exampleJson = null;
            exampleJson = "{\r\n  \"top_logprobs\" : 1,\r\n  \"instructions\" : \"Response_allOf_instructions\",\r\n  \"metadata\" : {\r\n    \"key\" : \"metadata\"\r\n  },\r\n  \"max_tool_calls\" : 1,\r\n  \"reasoning\" : {\r\n    \"summary\" : \"auto\",\r\n    \"effort\" : \"medium\",\r\n    \"generate_summary\" : \"auto\"\r\n  },\r\n  \"usage\" : {\r\n    \"input_tokens_details\" : {\r\n      \"cached_tokens\" : 4\r\n    },\r\n    \"total_tokens\" : 1,\r\n    \"output_tokens\" : 7,\r\n    \"input_tokens\" : 2,\r\n    \"output_tokens_details\" : {\r\n      \"reasoning_tokens\" : 1\r\n    }\r\n  },\r\n  \"created_at\" : 5.962133916683182,\r\n  \"safety_identifier\" : \"safety-identifier-1234\",\r\n  \"error\" : {\r\n    \"code\" : \"server_error\",\r\n    \"message\" : \"message\"\r\n  },\r\n  \"tools\" : [ {\r\n    \"name\" : \"name\",\r\n    \"description\" : \"description\",\r\n    \"type\" : \"function\",\r\n    \"strict\" : true,\r\n    \"parameters\" : {\r\n      \"key\" : \"\"\r\n    }\r\n  }, {\r\n    \"name\" : \"name\",\r\n    \"description\" : \"description\",\r\n    \"type\" : \"function\",\r\n    \"strict\" : true,\r\n    \"parameters\" : {\r\n      \"key\" : \"\"\r\n    }\r\n  } ],\r\n  \"top_p\" : 1,\r\n  \"output\" : [ {\r\n    \"role\" : \"assistant\",\r\n    \"id\" : \"id\",\r\n    \"type\" : \"message\",\r\n    \"content\" : [ {\r\n      \"annotations\" : [ {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      }, {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      } ],\r\n      \"text\" : \"text\",\r\n      \"type\" : \"output_text\",\r\n      \"logprobs\" : [ {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      }, {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      } ]\r\n    }, {\r\n      \"annotations\" : [ {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      }, {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      } ],\r\n      \"text\" : \"text\",\r\n      \"type\" : \"output_text\",\r\n      \"logprobs\" : [ {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      }, {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      } ]\r\n    } ],\r\n    \"status\" : \"in_progress\"\r\n  }, {\r\n    \"role\" : \"assistant\",\r\n    \"id\" : \"id\",\r\n    \"type\" : \"message\",\r\n    \"content\" : [ {\r\n      \"annotations\" : [ {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      }, {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      } ],\r\n      \"text\" : \"text\",\r\n      \"type\" : \"output_text\",\r\n      \"logprobs\" : [ {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      }, {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      } ]\r\n    }, {\r\n      \"annotations\" : [ {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      }, {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      } ],\r\n      \"text\" : \"text\",\r\n      \"type\" : \"output_text\",\r\n      \"logprobs\" : [ {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      }, {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      } ]\r\n    } ],\r\n    \"status\" : \"in_progress\"\r\n  } ],\r\n  \"previous_response_id\" : \"previous_response_id\",\r\n  \"temperature\" : 1,\r\n  \"tool_choice\" : \"none\",\r\n  \"service_tier\" : \"auto\",\r\n  \"model\" : \"gpt-4o\",\r\n  \"text\" : {\r\n    \"format\" : {\r\n      \"type\" : \"text\"\r\n    },\r\n    \"verbosity\" : \"medium\"\r\n  },\r\n  \"id\" : \"id\",\r\n  \"prompt_cache_key\" : \"prompt-cache-key-1234\",\r\n  \"truncation\" : \"disabled\",\r\n  \"incomplete_details\" : {\r\n    \"reason\" : \"max_output_tokens\"\r\n  },\r\n  \"parallel_tool_calls\" : true,\r\n  \"background\" : false,\r\n  \"output_text\" : \"output_text\",\r\n  \"user\" : \"user-1234\",\r\n  \"prompt\" : {\r\n    \"variables\" : {\r\n      \"key\" : \"ResponsePromptVariables_value\"\r\n    },\r\n    \"id\" : \"id\",\r\n    \"version\" : \"version\"\r\n  },\r\n  \"max_output_tokens\" : 6,\r\n  \"object\" : \"response\",\r\n  \"status\" : \"completed\"\r\n}";
            exampleJson = "Custom MIME type example not yet supported: text/event-stream";
            
            var example = exampleJson != null
            ? JsonSerializer.Deserialize<Response>(exampleJson)
            : default;
            //TODO: Change the data returned
            return new ObjectResult(example);
        }

        /// <summary>
        /// Delete a model response
        /// </summary>
        /// <remarks>Deletes a model response with the given ID. </remarks>
        /// <param name="responseId">The ID of the response to delete.</param>
        /// <response code="200">OK</response>
        /// <response code="404">Not Found</response>
        [HttpDelete]
        [Route("/v1/responses/{response_id}")]
        [Authorize]
        [ValidateModelState]
        [ProducesResponseType(statusCode: 404, type: typeof(Error))]
        public virtual IActionResult DeleteResponse([FromRoute (Name = "response_id")][Required]string responseId)
        {

            //TODO: Uncomment the next line to return response 200 or use other options such as return this.NotFound(), return this.BadRequest(..), ...
            // return StatusCode(200);
            //TODO: Uncomment the next line to return response 404 or use other options such as return this.NotFound(), return this.BadRequest(..), ...
            // return StatusCode(404, default);

            throw new NotImplementedException();
        }

        /// <summary>
        /// Get a model response
        /// </summary>
        /// <remarks>Retrieves a model response with the given ID. </remarks>
        /// <param name="responseId">The ID of the response to retrieve.</param>
        /// <param name="include">Additional fields to include in the response. See the &#x60;include&#x60; parameter for Response creation above for more information. </param>
        /// <param name="stream">If set to true, the model response data will be streamed to the client as it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format). See the [Streaming section below](https://platform.openai.com/docs/api-reference/responses-streaming) for more information. </param>
        /// <param name="startingAfter">The sequence number of the event after which to start streaming. </param>
        /// <param name="includeObfuscation">When true, stream obfuscation will be enabled. Stream obfuscation adds random characters to an &#x60;obfuscation&#x60; field on streaming delta events to normalize payload sizes as a mitigation to certain side-channel attacks. These obfuscation fields are included by default, but add a small amount of overhead to the data stream. You can set &#x60;include_obfuscation&#x60; to false to optimize for bandwidth if you trust the network links between your application and the OpenAI API. </param>
        /// <response code="200">OK</response>
        [HttpGet]
        [Route("/v1/responses/{response_id}")]
        [Authorize]
        [ValidateModelState]
        [ProducesResponseType(statusCode: 200, type: typeof(Response))]
        public virtual IActionResult GetResponse([FromRoute (Name = "response_id")][Required]string responseId, [FromQuery (Name = "include")]List<Includable>? include, [FromQuery (Name = "stream")]bool? stream, [FromQuery (Name = "starting_after")]int? startingAfter, [FromQuery (Name = "include_obfuscation")]bool? includeObfuscation)
        {

            //TODO: Uncomment the next line to return response 200 or use other options such as return this.NotFound(), return this.BadRequest(..), ...
            // return StatusCode(200, default);
            string exampleJson = null;
            exampleJson = "{\r\n  \"top_logprobs\" : 1,\r\n  \"instructions\" : \"Response_allOf_instructions\",\r\n  \"metadata\" : {\r\n    \"key\" : \"metadata\"\r\n  },\r\n  \"max_tool_calls\" : 1,\r\n  \"reasoning\" : {\r\n    \"summary\" : \"auto\",\r\n    \"effort\" : \"medium\",\r\n    \"generate_summary\" : \"auto\"\r\n  },\r\n  \"usage\" : {\r\n    \"input_tokens_details\" : {\r\n      \"cached_tokens\" : 4\r\n    },\r\n    \"total_tokens\" : 1,\r\n    \"output_tokens\" : 7,\r\n    \"input_tokens\" : 2,\r\n    \"output_tokens_details\" : {\r\n      \"reasoning_tokens\" : 1\r\n    }\r\n  },\r\n  \"created_at\" : 5.962133916683182,\r\n  \"safety_identifier\" : \"safety-identifier-1234\",\r\n  \"error\" : {\r\n    \"code\" : \"server_error\",\r\n    \"message\" : \"message\"\r\n  },\r\n  \"tools\" : [ {\r\n    \"name\" : \"name\",\r\n    \"description\" : \"description\",\r\n    \"type\" : \"function\",\r\n    \"strict\" : true,\r\n    \"parameters\" : {\r\n      \"key\" : \"\"\r\n    }\r\n  }, {\r\n    \"name\" : \"name\",\r\n    \"description\" : \"description\",\r\n    \"type\" : \"function\",\r\n    \"strict\" : true,\r\n    \"parameters\" : {\r\n      \"key\" : \"\"\r\n    }\r\n  } ],\r\n  \"top_p\" : 1,\r\n  \"output\" : [ {\r\n    \"role\" : \"assistant\",\r\n    \"id\" : \"id\",\r\n    \"type\" : \"message\",\r\n    \"content\" : [ {\r\n      \"annotations\" : [ {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      }, {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      } ],\r\n      \"text\" : \"text\",\r\n      \"type\" : \"output_text\",\r\n      \"logprobs\" : [ {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      }, {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      } ]\r\n    }, {\r\n      \"annotations\" : [ {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      }, {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      } ],\r\n      \"text\" : \"text\",\r\n      \"type\" : \"output_text\",\r\n      \"logprobs\" : [ {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      }, {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      } ]\r\n    } ],\r\n    \"status\" : \"in_progress\"\r\n  }, {\r\n    \"role\" : \"assistant\",\r\n    \"id\" : \"id\",\r\n    \"type\" : \"message\",\r\n    \"content\" : [ {\r\n      \"annotations\" : [ {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      }, {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      } ],\r\n      \"text\" : \"text\",\r\n      \"type\" : \"output_text\",\r\n      \"logprobs\" : [ {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      }, {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      } ]\r\n    }, {\r\n      \"annotations\" : [ {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      }, {\r\n        \"filename\" : \"filename\",\r\n        \"file_id\" : \"file_id\",\r\n        \"index\" : 5,\r\n        \"type\" : \"file_citation\"\r\n      } ],\r\n      \"text\" : \"text\",\r\n      \"type\" : \"output_text\",\r\n      \"logprobs\" : [ {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      }, {\r\n        \"top_logprobs\" : [ {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        }, {\r\n          \"logprob\" : 9.301444243932576,\r\n          \"bytes\" : [ 3, 3 ],\r\n          \"token\" : \"token\"\r\n        } ],\r\n        \"logprob\" : 2.3021358869347655,\r\n        \"bytes\" : [ 7, 7 ],\r\n        \"token\" : \"token\"\r\n      } ]\r\n    } ],\r\n    \"status\" : \"in_progress\"\r\n  } ],\r\n  \"previous_response_id\" : \"previous_response_id\",\r\n  \"temperature\" : 1,\r\n  \"tool_choice\" : \"none\",\r\n  \"service_tier\" : \"auto\",\r\n  \"model\" : \"gpt-4o\",\r\n  \"text\" : {\r\n    \"format\" : {\r\n      \"type\" : \"text\"\r\n    },\r\n    \"verbosity\" : \"medium\"\r\n  },\r\n  \"id\" : \"id\",\r\n  \"prompt_cache_key\" : \"prompt-cache-key-1234\",\r\n  \"truncation\" : \"disabled\",\r\n  \"incomplete_details\" : {\r\n    \"reason\" : \"max_output_tokens\"\r\n  },\r\n  \"parallel_tool_calls\" : true,\r\n  \"background\" : false,\r\n  \"output_text\" : \"output_text\",\r\n  \"user\" : \"user-1234\",\r\n  \"prompt\" : {\r\n    \"variables\" : {\r\n      \"key\" : \"ResponsePromptVariables_value\"\r\n    },\r\n    \"id\" : \"id\",\r\n    \"version\" : \"version\"\r\n  },\r\n  \"max_output_tokens\" : 6,\r\n  \"object\" : \"response\",\r\n  \"status\" : \"completed\"\r\n}";
            
            var example = exampleJson != null
            ? JsonSerializer.Deserialize<Response>(exampleJson)
            : default;
            //TODO: Change the data returned
            return new ObjectResult(example);
        }

        /// <summary>
        /// List input items
        /// </summary>
        /// <remarks>Returns a list of input items for a given response.</remarks>
        /// <param name="responseId">The ID of the response to retrieve input items for.</param>
        /// <param name="limit">A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20. </param>
        /// <param name="order">The order to return the input items in. Default is &#x60;desc&#x60;. - &#x60;asc&#x60;: Return the input items in ascending order. - &#x60;desc&#x60;: Return the input items in descending order. </param>
        /// <param name="after">An item ID to list items after, used in pagination. </param>
        /// <param name="before">An item ID to list items before, used in pagination. </param>
        /// <param name="include">Additional fields to include in the response. See the &#x60;include&#x60; parameter for Response creation above for more information. </param>
        /// <response code="200">OK</response>
        [HttpGet]
        [Route("/v1/responses/{response_id}/input_items")]
        [Authorize]
        [ValidateModelState]
        [ProducesResponseType(statusCode: 200, type: typeof(ResponseItemList))]
        public virtual IActionResult ListInputItems([FromRoute (Name = "response_id")][Required]string responseId, [FromQuery (Name = "limit")]int? limit, [FromQuery (Name = "order")]string? order, [FromQuery (Name = "after")]string? after, [FromQuery (Name = "before")]string? before, [FromQuery (Name = "include")]List<Includable>? include)
        {

            //TODO: Uncomment the next line to return response 200 or use other options such as return this.NotFound(), return this.BadRequest(..), ...
            // return StatusCode(200, default);
            string exampleJson = null;
            exampleJson = "{\r\n  \"first_id\" : \"first_id\",\r\n  \"data\" : [ {\r\n    \"role\" : \"user\",\r\n    \"id\" : \"id\",\r\n    \"type\" : \"message\",\r\n    \"content\" : [ {\r\n      \"text\" : \"text\",\r\n      \"type\" : \"input_text\"\r\n    }, {\r\n      \"text\" : \"text\",\r\n      \"type\" : \"input_text\"\r\n    } ],\r\n    \"status\" : \"in_progress\"\r\n  }, {\r\n    \"role\" : \"user\",\r\n    \"id\" : \"id\",\r\n    \"type\" : \"message\",\r\n    \"content\" : [ {\r\n      \"text\" : \"text\",\r\n      \"type\" : \"input_text\"\r\n    }, {\r\n      \"text\" : \"text\",\r\n      \"type\" : \"input_text\"\r\n    } ],\r\n    \"status\" : \"in_progress\"\r\n  } ],\r\n  \"last_id\" : \"last_id\",\r\n  \"has_more\" : true,\r\n  \"object\" : \"\"\r\n}";
            
            var example = exampleJson != null
            ? JsonSerializer.Deserialize<ResponseItemList>(exampleJson)
            : default;
            //TODO: Change the data returned
            return new ObjectResult(example);
        }
    }
}
